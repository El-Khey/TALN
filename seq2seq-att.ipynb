{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22731e38",
   "metadata": {},
   "source": [
    "# Attention Mechanisme Text to Text Translation : English to French\n",
    "\n",
    "This notebook trains a model for French to English translation. Using the attention mechanism, the model learns to align and translate French text to English text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47fcecf",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "We will start by importing the libraries we need for this project. You can install any missing libraries using the requirements.txt file provided or by running ``make install`` in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbb64c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%aimport utils.text_processing\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ahead-gilbert",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idriss/Documents/Github/Artorius/to-delete/TALN/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-18 16:45:55.414261: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-18 16:45:55.415383: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-18 16:45:55.439152: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-18 16:45:55.439724: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-18 16:45:55.886642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import digits\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from utils.text_processing import TextProcessor\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f9519",
   "metadata": {},
   "source": [
    "### Verify access to the GPU\n",
    "The following test applies only if you expect to be using a GPU, e.g., while running in a cloud environment with GPU support. Run the next cell, and verify that the device_type is \"GPU\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d302be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available:  []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:45:56.856345: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-18 16:45:56.856647: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"cuda available: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e1b59d",
   "metadata": {},
   "source": [
    "### Verify TensorFlow CUDA and cuDNN Versions\n",
    "Run the next cell to inspect which CUDA and cuDNN versions TensorFlow was built against:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "531b0af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.13.0\n",
      "CUDA version built: 11.8\n",
      "cuDNN version built: 8\n",
      "Detected GPU devices: []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "build_info = tf.sysconfig.get_build_info()\n",
    "print(\"CUDA version built:\", build_info.get(\"cuda_version\", \"Unknown\"))\n",
    "print(\"cuDNN version built:\", build_info.get(\"cudnn_version\", \"Unknown\"))\n",
    "print(\"Detected GPU devices:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9863892",
   "metadata": {},
   "source": [
    "We provide a in depth analysis of the data in the ``exploratory_analysis.ipynb`` notebook. We will not be doing any exploratory analysis in this notebook. Instead, we will focus on building our baseline model. So, let's start by importing the dataset we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326899b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Nicolas-BZRD/Parallel_Global_Voices_English_French\", split='train').to_pandas()\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd72c8",
   "metadata": {},
   "source": [
    "The actual data contains over 350,000 sentence-pairs. However, to speed up training for this notebook, we will only use a small portion of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Use the whole dataset (but it's too big for my computer)\n",
    "dataset = dataset.sample(n=50000, random_state=42)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-crash",
   "metadata": {},
   "source": [
    "## Text Pre-Processing\n",
    "\n",
    "The text pre-processing steps will be implemented in a class called ``TextPreprocessor``. This class will be used to clean and tokenize the text data. The class will also be used to convert the text to sequences and pad the sequences to a maximum length. This way we will be able to improve our model's without having to copy and paste the same code over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7168245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['en'] = TextProcessor(dataset, 'en').transform()\n",
    "dataset['fr'] = TextProcessor(dataset, 'fr').transform()\n",
    "\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c032ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only sentences with less than max_sequence_length words\n",
    "dataset = dataset[dataset['en'].str.split().str.len() <= max_sequence_length]\n",
    "dataset = dataset[dataset['fr'].str.split().str.len() <= max_sequence_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ccfcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e7bc9",
   "metadata": {},
   "source": [
    "### Text to Sequence Conversion\n",
    "\n",
    "To feed our data to a Seq2Seq model, we will have to convert both the input and the output sentences into integer sequences of fixed length. Check the exploratory data analysis notebook to see the distribution of the lengths of the sentences in the dataset. Based on that, we decided to fix the maximum length of each sentence to 20 since the average length of the sentences in the dataset is around 20.\n",
    "\n",
    "We will use the ``Tokenizer`` class from the ``tensorflow.keras.preprocessing.text`` module to tokenize the text data. The ``Tokenizer`` class will also be used to convert the text to sequences. We will use the ``pad_sequences`` function from the same module to pad the sequences to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082de051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(lines, max_vocab_size=100000):\n",
    "    tokenizer = Tokenizer(filters='', num_words=max_vocab_size)\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    seq = pad_sequences(seq, maxlen=length, padding='post', truncating='post')\n",
    "    return seq\n",
    "\n",
    "def decode_sequences(tokenizer, sequence):\n",
    "    text = tokenizer.sequences_to_texts([sequence])[0]\n",
    "    return text\n",
    "\n",
    "def get_most_common_words(tokenizer, n=10):\n",
    "    word_counts = sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    return word_counts[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f58b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16887038",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['en'] = dataset['en'].apply(lambda s: '<start> ' + s + ' <end>')\n",
    "dataset['fr'] = dataset['fr'].apply(lambda s: '<start> ' + s + ' <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9445480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the English sentences\n",
    "eng_tokenizer = tokenization(dataset[\"en\"], max_vocab_size=max_vocab_size)\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "# Tokenize the French sentences\n",
    "fr_tokenizer = tokenization(dataset[\"fr\"], max_vocab_size=max_vocab_size)\n",
    "fr_vocab_size = len(fr_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('French Vocabulary Size: %d' % fr_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4fc052",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most common words in English: \", get_most_common_words(eng_tokenizer))\n",
    "print(\"Most common words in French: \", get_most_common_words(fr_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ea319c",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "We will now split the data into train and test set for model training and evaluation, respectively. We will use the ``train_test_split`` function from the ``sklearn.model_selection`` module to split the data. We will use 10% of the data for testing and the rest for training. We will also set the ``random_state`` parameter to 42 to ensure reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae478a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8cc1a",
   "metadata": {},
   "source": [
    "It's time to encode the sentences. We will encode French sentences as the input sequences and English sentences as the target sequences. It will be done for both tra and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ca55db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(fr_tokenizer, max_sequence_length, dataset[\"fr\"])\n",
    "trainY = encode_sequences(eng_tokenizer, max_sequence_length, dataset[\"en\"])\n",
    "\n",
    "# prepare validation data\n",
    "testX = encode_sequences(fr_tokenizer, max_sequence_length, test_data[\"fr\"])\n",
    "testY = encode_sequences(eng_tokenizer, max_sequence_length, test_data[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX.reshape((-1, trainX.shape[1], 1))\n",
    "testX = testX.reshape((-1, testX.shape[1], 1))\n",
    "\n",
    "trainY = trainY.reshape((-1, trainY.shape[1], 1))\n",
    "testY = testY.reshape((-1, testY.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a3f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec4044",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7564baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6242012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentences_without_start_end(df, column):\n",
    "    # Returns rows where the sentence does not start with <start> or does not end with <end>\n",
    "    mask = ~df[column].str.startswith('<start>') | ~df[column].str.endswith('<end>')\n",
    "    return df[mask]\n",
    "\n",
    "# Example usage for English and French columns:\n",
    "invalid_en = find_sentences_without_start_end(train_data, 'en')\n",
    "invalid_fr = find_sentences_without_start_end(train_data, 'fr')\n",
    "print(\"English sentences without <start>/<end>:\", len(invalid_en))\n",
    "print(\"French sentences without <start>/<end>:\", len(invalid_fr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f6575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode sample sequences from the training set\n",
    "for i in range(1500):\n",
    "    english = decode_sequences(eng_tokenizer, trainY[i, : ,0])\n",
    "    french = decode_sequences(fr_tokenizer, trainX[i, : ,0])\n",
    "    print('English: ', english, len(english.split()))\n",
    "    print('French: ', french , len(french.split()))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dabb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('<start>' in fr_tokenizer.word_index)\n",
    "print(fr_tokenizer.word_index.get('<start>'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-weather",
   "metadata": {},
   "source": [
    "## Encoder Decoder with Attention mechanism \n",
    "\n",
    "![Attention](../images/attention.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder_units = encoder_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(encoder_units, \n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True,                                      \n",
    "                                      recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        # pass the input x to the embedding layer\n",
    "        x = self.embedding(x)\n",
    "        # pass the embedding and the hidden state to GRU\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoder_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        \n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        \n",
    "        # hidden state shape == (batch_size, hidden size)\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        # enc_output (batch_size, max?_lenght, encoder_units) ,enc_hidden (batch_size, encoder_units)\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden) \n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        # dec_input(batch_size, 1)\n",
    "        dec_input = tf.expand_dims([fr_tokenizer.word_index['<start>']] * BATCH_SIZE, 1) \n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1) # using teacher forcing\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "# Create data in memeory and shuffles the data in the batches\n",
    "dataset=tf.data.Dataset.from_tensor_slices((trainX.reshape((-1, trainX.shape[-2])), trainY.reshape((-1, trainY.shape[1])))).shuffle(BATCH_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(fr_vocab_size, embedding_dim=embedding_dim, encoder_units=units, batch_size=BATCH_SIZE)\n",
    "decoder = Decoder(vocab_size=eng_vocab_size, embedding_dim=embedding_dim, dec_units=units, batch_sz=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '../models/encode_decoder_attention_training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "steps_per_epoch = len(trainX) // BATCH_SIZE\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    for (batch, (inp, targ)) in tqdm(enumerate(dataset.take(steps_per_epoch)), total=steps_per_epoch):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-catalyst",
   "metadata": {},
   "source": [
    "### Make Prediction using the trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentences, source_sentence_tokenizer=fr_tokenizer, target_sentence_tokenizer=eng_tokenizer): \n",
    "    max_length = trainX.shape[1]\n",
    "    attention_plot = np.zeros((max_length, max_length))\n",
    "\n",
    "    sentence = TextProcessor(sentences, 'fr').process(sentences)\n",
    "\n",
    "    inputs = [source_sentence_tokenizer.word_index[w] for w in sentences.split()]\n",
    "    inputs = pad_sequences([inputs], maxlen=max_length, padding=\"post\")\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = \"\"\n",
    "    \n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['<start>']], 0)\n",
    "    \n",
    "    for t in range(max_length):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights =  tf.reshape(attention_weights,(-1, ))\n",
    "        attention_plot [t] = attention_weights.numpy()\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += target_sentence_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if target_sentence_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap=\"viridis\")\n",
    "    \n",
    "    sentence = sentence.split()\n",
    "    predicted_sentence = predicted_sentence.split()\n",
    "    \n",
    "    # Add <PAD> token if sentence or predicted_sentence is shorter than attention matrix\n",
    "    if len(sentence) < attention.shape[1]:\n",
    "        sentence += [\"<PAD>\"] * (attention.shape[1] - len(sentence))\n",
    "        \n",
    "    if len(predicted_sentence) < attention.shape[0]:\n",
    "        predicted_sentence += [\"<PAD>\"] * (attention.shape[0] - len(predicted_sentence))\n",
    "    \n",
    "    fontdict = {\"fontsize\": 14}\n",
    "    ax.set_xticklabels([' '] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([' '] + predicted_sentence, fontdict=fontdict)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    predicted_text, sentence, attention_plot = predict(sentence)\n",
    "        \n",
    "    attention_plot = attention_plot[:len(predicted_text.split(' ')), :len(sentence.split(' '))]\n",
    "    return predicted_text, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-ground",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_id = np.random.randint(0, testX.shape[0])\n",
    "sentence = ' '.join([fr_tokenizer.index_word[i[0]] for i in testX[random_id] if i[0] != 0][1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_text, attention_plot = translate(sentence)\n",
    "\n",
    "print('Input:        ', sentence)\n",
    "print('Predicted:   ', predicted_text)\n",
    "print('Ground Truth: ', decode_sequences(eng_tokenizer, testY[random_id, : ,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86687643",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention(attention_plot, sentence, predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349c7816",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "references = []\n",
    "candidates = []\n",
    "\n",
    "for i in tqdm(range(testX.shape[0]//2)):\n",
    "    textX_decoded = decode_sequences(fr_tokenizer, testX[i, : ,0])\n",
    "    testY_decoded = decode_sequences(eng_tokenizer, testY[i, : ,0])\n",
    "    candidate = translate(textX_decoded)[0].replace('<end>', '').replace('<start>', '').strip()\n",
    "    \n",
    "    data.append({\n",
    "        'Context': textX_decoded,\n",
    "        'Reference': testY_decoded,\n",
    "        'Candidate': candidate,\n",
    "        'length': len(textX_decoded.split())\n",
    "    })\n",
    "    \n",
    "    references.append([testY_decoded])\n",
    "    candidates.append(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d65116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into small dataset based on the sentences length\n",
    "length_ranges = [(1, 5), (6, 10), (11, 15), (16, 20), (21, 30), (31, 40), (41, 60), (61, float('inf'))]\n",
    "\n",
    "small_datasets = {}\n",
    "for min_len, max_len in length_ranges:\n",
    "    filtered_examples = [example for example in data if example['length'] >= min_len and example['length'] <= max_len]\n",
    "    small_datasets[f'dataset_{min_len}_{max_len}'] = filtered_examples\n",
    "\n",
    "samples_per_range = []\n",
    "for key, dataset in small_datasets.items():\n",
    "    samples_per_range.append(len(dataset))\n",
    "    print(f\"{key}: {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db948741",
   "metadata": {},
   "source": [
    "### Troubleshooting GPU Detection\n",
    "\n",
    "Your system shows NVIDIA driver 535.230.02 with CUDA 12.2, but TensorFlow 2.19.0 is built against CUDA 12.5 and cuDNN 9, causing a version mismatch. To enable GPU support, choose one of the following:\n",
    "\n",
    "1. Install CUDA Toolkit 12.5 and cuDNN 9:\n",
    "   - Download and install CUDA 12.5 from NVIDIA.\n",
    "   - Download cuDNN 9 for CUDA 12.5 and place headers/libs under `/usr/local/cuda-12.5/`.\n",
    "   - Add to your `~/.bashrc`:\n",
    "     ```bash\n",
    "     export PATH=/usr/local/cuda-12.5/bin:$PATH\n",
    "     export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64:$LD_LIBRARY_PATH\n",
    "     ```\n",
    "   - Reload your shell or restart.\n",
    "   - Reinstall or upgrade TensorFlow:\n",
    "     ```bash\n",
    "     pip install --upgrade tensorflow\n",
    "     ```\n",
    "\n",
    "2. Or install a TensorFlow build matching CUDA 12.2:\n",
    "   ```bash\n",
    "   pip install tensorflow==2.13\n",
    "   ```\n",
    "\n",
    "3. Restart the notebook kernel, then rerun the GPU check:\n",
    "   ```python\n",
    "   import tensorflow as tf\n",
    "   print(\"Detected GPU devices:\", tf.config.list_physical_devices('GPU'))\n",
    "   ```\n",
    "4. Ensure the CUDA toolkit runtime is installed and accessible:\n",
    "    - On Ubuntu/Debian, for CUDA 12.2:\n",
    "      ```bash\n",
    "      sudo apt-get update\n",
    "      sudo apt-get install -y cuda-toolkit-12-2\n",
    "      ```\n",
    "    - Confirm `/usr/local/cuda` points to CUDA 12.2:\n",
    "      ```bash\n",
    "      ls -l /usr/local/cuda\n",
    "      nvcc --version\n",
    "      ```\n",
    "    - If needed, create or update the symlink:\n",
    "      ```bash\n",
    "      sudo ln -sf /usr/local/cuda-12.2 /usr/local/cuda\n",
    "      sudo ldconfig\n",
    "      ```\n",
    "    - Reload your shell and rerun the GPU check cell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
