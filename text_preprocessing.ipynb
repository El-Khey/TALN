{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2722fc75",
   "metadata": {},
   "source": [
    "# Text Data Preprocessing Pipeline\n",
    "\n",
    "This notebook implements a comprehensive text preprocessing pipeline for natural language processing tasks. The pipeline handles data loading, text extraction, and various normalization steps to prepare textual data for further analysis or model training.\n",
    "\n",
    "## Objective\n",
    "- Load and consolidate text data from multiple CSV files\n",
    "- Clean and normalize text content \n",
    "- Prepare standardized dataset for NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4c38f4",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "\n",
    "- Enables autoreload extension to automatically reload modified modules\n",
    "- Imports the custom TextProcessor class from utils\n",
    "- Loads required data manipulation libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "07bc8922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%aimport utils.text_processing\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0b7945c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.text_processing import TextProcessor\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604c245",
   "metadata": {},
   "source": [
    "## Load CSV Files\n",
    "\n",
    "The pipeline scans for CSV files in the data directory:\n",
    "1. Uses glob to find all .csv files\n",
    "2. Validates file structure by inspecting first file\n",
    "3. Ensures consistency in data format across files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "42a4fd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 CSV files\n"
     ]
    }
   ],
   "source": [
    "csv_files = glob.glob('data/raw/*.csv')\n",
    "print(f'Found {len(csv_files)} CSV files')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1dc66c",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "For each CSV file:\n",
    "1. Extracts the 'content' column containing text messages\n",
    "2. Removes empty entries\n",
    "3. Maintains data quality by filtering invalid entries\n",
    "4. Tracks processing statistics for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e57b69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data/raw/shadow-slave_page_24.csv: 996 messages\n",
      "Processed data/raw/shadow-slave_page_17.csv: 991 messages\n",
      "Processed data/raw/shadow-slave_page_25.csv: 993 messages\n",
      "Processed data/raw/shadow-slave_page_23.csv: 989 messages\n",
      "Processed data/raw/shadow-slave_page_18.csv: 995 messages\n",
      "Processed data/raw/shadow-slave_page_21.csv: 990 messages\n",
      "Processed data/raw/shadow-slave_page_16.csv: 984 messages\n",
      "Processed data/raw/shadow-slave_page_19.csv: 990 messages\n",
      "Processed data/raw/shadow-slave_page_20.csv: 991 messages\n",
      "Processed data/raw/shadow-slave_page_26.csv: 994 messages\n",
      "Processed data/raw/shadow-slave_page_27.csv: 992 messages\n",
      "Processed data/raw/shadow-slave_page_22.csv: 994 messages\n",
      "Processed data/raw/shadow-slave_page_28.csv: 599 messages\n"
     ]
    }
   ],
   "source": [
    "# Initialize list to store all DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Process each CSV file\n",
    "for file in csv_files:\n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # keep only the content column\n",
    "    if 'content' not in df.columns:\n",
    "        print(f'Skipping {file}: no content column')\n",
    "        continue\n",
    "    \n",
    "    df = df[['content']]\n",
    "    df = df[df['content'].str.len() > 0]\n",
    "    df.rename(columns={'content': 'fr'}, inplace=True)\n",
    "    \n",
    "    dataframes.append(df)\n",
    "    print(f'Processed {file}: {len(df)} messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2778d592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final DataFrame shape: (11495, 1)\n",
      "\n",
      "Sample of processed texts:\n",
      "                                                  fr\n",
      "0  <start>il a pas pu faire grand chose qui puiss...\n",
      "1  <start>jpense juste qu'il aime pas qu'on s'int...\n",
      "2  <start>par contre pour que sunny ne veulent pa...\n",
      "3  <start>c'est pas simplement du a son effacemen...\n",
      "4  <start>psq la le fait qu'ils oublient a chaque...\n"
     ]
    }
   ],
   "source": [
    "final_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "final_df = final_df.drop_duplicates()\n",
    "final_df['fr'] = TextProcessor(final_df, 'fr').transform()\n",
    "\n",
    "\n",
    "print(f'\\nFinal DataFrame shape: {final_df.shape}')\n",
    "print('\\nSample of processed texts:')\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "04396d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed data saved to ./data/cleaned/fr_processed_data.csv\n"
     ]
    }
   ],
   "source": [
    "output_file = './data/cleaned/fr_processed_data.csv'\n",
    "final_df.to_csv(output_file, index=False)\n",
    "print(f'\\nProcessed data saved to {output_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecf1cdc",
   "metadata": {},
   "source": [
    "## Data Export\n",
    "\n",
    "Final processing steps:\n",
    "1. Combines all processed DataFrames\n",
    "2. Removes any duplicate entries\n",
    "3. Exports to CSV format for downstream tasks\n",
    "4. Preserves both original and processed versions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
