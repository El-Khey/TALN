{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Text to Text Translation : English to French\n",
    "\n",
    "This notebook trains a sequence to sequence (seq2seq) model for English to French translation. This model will be our **baseline** model, which we will then improve upon by adding attention and other features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "We will start by importing the libraries we need for this project. You can install any missing libraries using the requirements.txt file provided or by running ``make install`` in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport utils.text_processing\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 21:55:00.494982: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-17 21:55:00.504813: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747511700.515400   67179 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747511700.518600   67179 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747511700.526894   67179 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747511700.526908   67179 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747511700.526910   67179 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747511700.526911   67179 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-17 21:55:00.530348: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from utils.text_processing import TextProcessor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify access to the GPU\n",
    "The following test applies only if you expect to be using a GPU, e.g., while running in a cloud environment with GPU support. Run the next cell, and verify that the device_type is \"GPU\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available:  []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1747511702.002262   67179 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"cuda available: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a in depth analysis of the data in the ``exploratory_analysis.ipynb`` notebook. We will not be doing any exploratory analysis in this notebook. Instead, we will focus on building our baseline model. So, let's start by importing the dataset we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fr",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "en",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "7e5ec8c2-6030-46f3-ac19-f850908d705f",
       "rows": [
        [
         "0",
         "<start>il a pas pu faire grand chose qui puisse choquer cassie<end>",
         "<start>he could not do much that could shock cassie<end>"
        ],
        [
         "1",
         "<start>jpense juste qu'il aime pas qu'on s'introduise dans sa tete<end>",
         "<start>i just think he does not like us getting into his head<end>"
        ],
        [
         "2",
         "<start>par contre pour que sunny ne veulent pas que cassie voit c'est souvenir des derniere annee il a vraiment du faire des dinguerie<end>",
         "<start>on the other hand so that sunny does not want cassie to see it is memory of the last year he really had to do some crazy things<end>"
        ],
        [
         "3",
         "<start>c'est pas simplement du a son effacement du destin ils devraient etre en mesure de faire le lien mais si l'autre piaf a des bouts de pouvoir du demon de l'oubli ca expliquerait tout<end>",
         "<start>it is not just because of its erasure of destiny they should be able to make the connection but if the other piaf has some bits of power of the demon of oblivion that would explain everything<end>"
        ],
        [
         "4",
         "<start>psq la le fait qu'ils oublient a chaque fois<end>",
         "<start>psq the fact that they forget every time<end>"
        ],
        [
         "5",
         "<start>possible<end>",
         "<start>possible<end>"
        ],
        [
         "6",
         "<start>il aurait pas voler des pouvoirs au demon de l'oubli<end>",
         "<start>he would not steal powers from the demon of oblivion<end>"
        ],
        [
         "7",
         "<start>le piaf de merde la<end>",
         "<start>the shitty piaf there<end>"
        ],
        [
         "8",
         "<start>par contre dcp<end>",
         "<start>but dcp<end>"
        ],
        [
         "9",
         "<start>elle sait juste que c'est lui l'anomalie<end>",
         "<start>she just knows he is the anomaly<end>"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fr</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt;il a pas pu faire grand chose qui puisse choquer cassie&lt;end&gt;</td>\n",
       "      <td>&lt;start&gt;he could not do much that could shock cassie&lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt;jpense juste qu'il aime pas qu'on s'introduise dans sa tete&lt;end&gt;</td>\n",
       "      <td>&lt;start&gt;i just think he does not like us getting into his head&lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt;par contre pour que sunny ne veulent pas que cassie voit c'est souvenir des derniere annee il a vraiment du faire des dinguerie&lt;end&gt;</td>\n",
       "      <td>&lt;start&gt;on the other hand so that sunny does not want cassie to see it is memory of the last year he really had to do some crazy things&lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt;c'est pas simplement du a son effacement du destin ils devraient etre en mesure de faire le lien mais si l'autre piaf a des bouts de pouvoir du demon de l'oubli ca expliquerait tout&lt;end&gt;</td>\n",
       "      <td>&lt;start&gt;it is not just because of its erasure of destiny they should be able to make the connection but if the other piaf has some bits of power of the demon of oblivion that would explain everythi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt;psq la le fait qu'ils oublient a chaque fois&lt;end&gt;</td>\n",
       "      <td>&lt;start&gt;psq the fact that they forget every time&lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;start&gt;possible&lt;end&gt;</td>\n",
       "      <td>&lt;start&gt;possible&lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;start&gt;il aurait pas voler des pouvoirs au demon de l'oubli&lt;end&gt;</td>\n",
       "      <td>&lt;start&gt;he would not steal powers from the demon of oblivion&lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;start&gt;le piaf de merde la&lt;end&gt;</td>\n",
       "      <td>&lt;start&gt;the shitty piaf there&lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;start&gt;par contre dcp&lt;end&gt;</td>\n",
       "      <td>&lt;start&gt;but dcp&lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;start&gt;elle sait juste que c'est lui l'anomalie&lt;end&gt;</td>\n",
       "      <td>&lt;start&gt;she just knows he is the anomaly&lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                  fr  \\\n",
       "0                                                                                                                                <start>il a pas pu faire grand chose qui puisse choquer cassie<end>   \n",
       "1                                                                                                                            <start>jpense juste qu'il aime pas qu'on s'introduise dans sa tete<end>   \n",
       "2                                                        <start>par contre pour que sunny ne veulent pas que cassie voit c'est souvenir des derniere annee il a vraiment du faire des dinguerie<end>   \n",
       "3  <start>c'est pas simplement du a son effacement du destin ils devraient etre en mesure de faire le lien mais si l'autre piaf a des bouts de pouvoir du demon de l'oubli ca expliquerait tout<end>   \n",
       "4                                                                                                                                           <start>psq la le fait qu'ils oublient a chaque fois<end>   \n",
       "5                                                                                                                                                                               <start>possible<end>   \n",
       "6                                                                                                                                   <start>il aurait pas voler des pouvoirs au demon de l'oubli<end>   \n",
       "7                                                                                                                                                                    <start>le piaf de merde la<end>   \n",
       "8                                                                                                                                                                         <start>par contre dcp<end>   \n",
       "9                                                                                                                                               <start>elle sait juste que c'est lui l'anomalie<end>   \n",
       "\n",
       "                                                                                                                                                                                                        en  \n",
       "0                                                                                                                                                 <start>he could not do much that could shock cassie<end>  \n",
       "1                                                                                                                                       <start>i just think he does not like us getting into his head<end>  \n",
       "2                                                              <start>on the other hand so that sunny does not want cassie to see it is memory of the last year he really had to do some crazy things<end>  \n",
       "3  <start>it is not just because of its erasure of destiny they should be able to make the connection but if the other piaf has some bits of power of the demon of oblivion that would explain everythi...  \n",
       "4                                                                                                                                                     <start>psq the fact that they forget every time<end>  \n",
       "5                                                                                                                                                                                     <start>possible<end>  \n",
       "6                                                                                                                                         <start>he would not steal powers from the demon of oblivion<end>  \n",
       "7                                                                                                                                                                        <start>the shitty piaf there<end>  \n",
       "8                                                                                                                                                                                      <start>but dcp<end>  \n",
       "9                                                                                                                                                             <start>she just knows he is the anomaly<end>  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('./data/cleaned/fr_en_processed_data.csv')\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual data contains over 350,000 sentence-pairs. However, to speed up training for this notebook, we will only use a small portion of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11187, 2)\n"
     ]
    }
   ],
   "source": [
    "# TODO : Use the whole dataset (but it's too big for my computer)\n",
    "# dataset = dataset.head(n=50000)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-Processing\n",
    "\n",
    "The text pre-processing steps will be implemented in a class called ``TextPreprocessor``. This class will be used to clean and tokenize the text data. The class will also be used to convert the text to sequences and pad the sequences to a maximum length. This way we will be able to improve our model's without having to copy and paste the same code over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fr",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "en",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "60d04d6a-4401-49bb-b37f-fe75cad52dc5",
       "rows": [
        [
         "0",
         "il a pas pu faire grand chose qui puisse choquer cassie",
         "he could not do much that could shock cassie"
        ],
        [
         "1",
         "jpense juste qu'il aime pas qu'on s'introduise dans sa tete",
         "i just think he does not like us getting into his head"
        ],
        [
         "2",
         "par contre pour que sunny ne veulent pas que cassie voit c'est souvenir des derniere annee il a vraiment du",
         "on the other hand so that sunny does not want cassie to see it is memory of the last year"
        ],
        [
         "3",
         "c'est pas simplement du a son effacement du destin ils devraient etre en mesure de faire le lien mais si",
         "it is not just because of its erasure of destiny they should be able to make the connection but if"
        ],
        [
         "4",
         "psq la le fait qu'ils oublient a chaque fois",
         "psq the fact that they forget every time"
        ],
        [
         "5",
         "possible",
         "possible"
        ],
        [
         "6",
         "il aurait pas voler des pouvoirs au demon de l'oubli",
         "he would not steal powers from the demon of oblivion"
        ],
        [
         "7",
         "le piaf de merde la",
         "the shitty piaf there"
        ],
        [
         "8",
         "par contre dcp",
         "but dcp"
        ],
        [
         "9",
         "elle sait juste que c'est lui l'anomalie",
         "she just knows he is the anomaly"
        ],
        [
         "10",
         "elle peut tjrs pas se souvenir que c'est lui son ancien compagnon etc",
         "she cannot remember that he is her old companion etc"
        ],
        [
         "11",
         "et comme elle a compris que y'a une anomalie avec sunny elle a fait le lien que sunny anomalie",
         "and as she understood that there was an anomaly with sunny she made the connection that sunny anomaly"
        ],
        [
         "12",
         "elle a oublie mais grace a son the elle a compris qu'elle a oublie",
         "she forgot but thanks to her the she realized she forgot"
        ],
        [
         "13",
         "c'est justement comme ca qu'elle a compris",
         "that is just how she understood"
        ],
        [
         "14",
         "mais dcp quand sunny le dis directement elle oublie tt mais quand elle le comprend toute seule elle retient cest",
         "but dcp when sunny says it directly she forgets it but when she understands it alone she retains it a"
        ],
        [
         "15",
         "mais quand meme cassie qui parce que son the est instant devenu froid elle comprend que elle a en face",
         "but when the same cassie who because her the moment became cold she understands that she has in front of"
        ],
        [
         "16",
         "tu t'es autoproclame tu n'etais pas la lors de la creation",
         "you self-proclaimed you were not there at creation"
        ],
        [
         "17",
         "shibalaugh",
         "shibalaugh"
        ],
        [
         "18",
         "noublie pas tu nest que le premier alors que je suis le meilleurs",
         "do not forget you are only the first when i am the best"
        ],
        [
         "19",
         "cassie mais quelle genie bordel de merde",
         "cassie but what a fucking bitch"
        ],
        [
         "20",
         "sunny et cassie qui vont faire le casse du siecle",
         "sunny and cassie who will break the century"
        ],
        [
         "21",
         "cassie je suis amoureux",
         "i am in love"
        ],
        [
         "22",
         "c'est pas ma femme pour rien",
         "she is not my wife for nothing"
        ],
        [
         "23",
         "cassie iq",
         "cassia iq"
        ],
        [
         "24",
         "c pas le meme lvl on va dire",
         "c not the same lvl we will say"
        ],
        [
         "25",
         "les attributs divin cest un autre level putain",
         "divine attributes is another fucking level"
        ],
        [
         "26",
         "en meme tps que veut tu faire contre un cafard transcendant qui se clone",
         "in the same tps you want to do against a transcendent cockroach that clones itself"
        ],
        [
         "27",
         "reel",
         "real"
        ],
        [
         "28",
         "enft en dehors de la nuke de nephis personne peut rien faire contre ce type les souverains compte pas",
         "but outside the nuke of nephis no one can do anything against this guy sovereigns do not count"
        ],
        [
         "29",
         "c son faux style de combat sah",
         "c his false fighting style sah"
        ],
        [
         "30",
         "si un clone est capable de ca",
         "if a clone is capable of this"
        ],
        [
         "31",
         "mais le combat contre nephis jme dis",
         "but the fight against nephis"
        ],
        [
         "32",
         "meme ca cest trop ingenieux",
         "even that is too ingenious"
        ],
        [
         "33",
         "a part celui de rain il est dans labu lui",
         "aside from the one of brass he is in the bush"
        ],
        [
         "34",
         "jai jure il font trop le taff",
         "i swear they do too much taff"
        ],
        [
         "35",
         "les clones de sunny sont trop trop forts",
         "sunny clones are too strong"
        ],
        [
         "36",
         "par contre force au reflet quil a envoye chez sunny",
         "on the other hand the reflection he sent to sunny"
        ],
        [
         "37",
         "jespere quil va pas trop tarder a agir",
         "i hope he is not too soon to act"
        ],
        [
         "38",
         "hate de lintroduction dasterion aussi",
         "haste of the introduction dasterion also"
        ],
        [
         "39",
         "mordret en saint il doit etre tellement fort ce mec",
         "it must be so loud that this guy"
        ],
        [
         "40",
         "jai instant compris jetait choque",
         "i realized moment was shocking"
        ],
        [
         "41",
         "elle sait pas que ya un fou furieux qui traine dedans",
         "she does not know there is a madman hanging in there"
        ],
        [
         "42",
         "rain qui comprend pas pourquoi sunny a peur des miroirs jsuis mort",
         "rain that does not understand why sunny is afraid of mirrors i am dead"
        ],
        [
         "43",
         "surtout que cassie a fait ca pour avoir une arme comme elle le dit dans les derniers chap",
         "especially that cassie did this to have a gun as she says in the last chapter"
        ],
        [
         "44",
         "par contre lutilisation du pouvoir de mordret par le domaine de song est mastermind jai jure",
         "against the use of mordert power by the domain of song is mastermind jai jure"
        ],
        [
         "45",
         "mais entre kai et jet si ils doivent choisir ils hesiteront pas",
         "but between kai and jet if they have to choose they will not hesite"
        ],
        [
         "46",
         "jet etait lamie de sunny",
         "jet was half of sunny"
        ],
        [
         "47",
         "et comme jet est sympa ils sont en bons termes",
         "and as jet is nice they are on good terms"
        ],
        [
         "48",
         "oe pas faux",
         "o not false"
        ],
        [
         "49",
         "mordret et jet cest des ajouts temporaires",
         "mordret and jet is temporary additions"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 11187
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fr</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>il a pas pu faire grand chose qui puisse choquer cassie</td>\n",
       "      <td>he could not do much that could shock cassie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jpense juste qu'il aime pas qu'on s'introduise dans sa tete</td>\n",
       "      <td>i just think he does not like us getting into his head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>par contre pour que sunny ne veulent pas que cassie voit c'est souvenir des derniere annee il a vraiment du</td>\n",
       "      <td>on the other hand so that sunny does not want cassie to see it is memory of the last year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c'est pas simplement du a son effacement du destin ils devraient etre en mesure de faire le lien mais si</td>\n",
       "      <td>it is not just because of its erasure of destiny they should be able to make the connection but if</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>psq la le fait qu'ils oublient a chaque fois</td>\n",
       "      <td>psq the fact that they forget every time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11182</th>\n",
       "      <td>genre une cohorte soude de saints qui ressorte d'un cauchemar surnaturel avec des memoires de fou furieux etc</td>\n",
       "      <td>like a soda cohort of saints that emerges from a supernatural nightmare with crazy memories etc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11183</th>\n",
       "      <td>jpense pas mais comme la famille de valor</td>\n",
       "      <td>i do not think but like the valor family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11184</th>\n",
       "      <td>le post eme cauchemard va etre fou moi je le dis</td>\n",
       "      <td>the post-eme nightmare will be crazy me i say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11185</th>\n",
       "      <td>et qu'ils soient enfin vraiment acteur de l'histoire</td>\n",
       "      <td>and they are finally really an actor in history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11186</th>\n",
       "      <td>je pref large la sortie instant jai envie de decouvrir comment le monde plonge dans le royaume des reves</td>\n",
       "      <td>i pref wide the exit moment i wanted to discover how the world plunges into the realm of dreams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11187 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                  fr  \\\n",
       "0                                                            il a pas pu faire grand chose qui puisse choquer cassie   \n",
       "1                                                        jpense juste qu'il aime pas qu'on s'introduise dans sa tete   \n",
       "2        par contre pour que sunny ne veulent pas que cassie voit c'est souvenir des derniere annee il a vraiment du   \n",
       "3           c'est pas simplement du a son effacement du destin ils devraient etre en mesure de faire le lien mais si   \n",
       "4                                                                       psq la le fait qu'ils oublient a chaque fois   \n",
       "...                                                                                                              ...   \n",
       "11182  genre une cohorte soude de saints qui ressorte d'un cauchemar surnaturel avec des memoires de fou furieux etc   \n",
       "11183                                                                      jpense pas mais comme la famille de valor   \n",
       "11184                                                               le post eme cauchemard va etre fou moi je le dis   \n",
       "11185                                                           et qu'ils soient enfin vraiment acteur de l'histoire   \n",
       "11186       je pref large la sortie instant jai envie de decouvrir comment le monde plonge dans le royaume des reves   \n",
       "\n",
       "                                                                                                       en  \n",
       "0                                                            he could not do much that could shock cassie  \n",
       "1                                                  i just think he does not like us getting into his head  \n",
       "2               on the other hand so that sunny does not want cassie to see it is memory of the last year  \n",
       "3      it is not just because of its erasure of destiny they should be able to make the connection but if  \n",
       "4                                                                psq the fact that they forget every time  \n",
       "...                                                                                                   ...  \n",
       "11182     like a soda cohort of saints that emerges from a supernatural nightmare with crazy memories etc  \n",
       "11183                                                            i do not think but like the valor family  \n",
       "11184                                                       the post-eme nightmare will be crazy me i say  \n",
       "11185                                                     and they are finally really an actor in history  \n",
       "11186     i pref wide the exit moment i wanted to discover how the world plunges into the realm of dreams  \n",
       "\n",
       "[11187 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REMOVE <start> and <end> tokens\n",
    "dataset['fr'] = dataset['fr'].apply(lambda x: x.replace('<start>', '').replace('<end>', '').strip())\n",
    "dataset['en'] = dataset['en'].apply(lambda x: x.replace('<start>', '').replace('<end>', '').strip())\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fr",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "en",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "8a6ecfef-1807-4cf2-ae92-c434ab856075",
       "rows": [
        [
         "0",
         "il a pas pu faire grand chose qui puisse choquer cassie",
         "he could not do much that could shock cassie"
        ],
        [
         "1",
         "jpense juste qu'il aime pas qu'on s'introduise dans sa tete",
         "i just think he does not like us getting into his head"
        ],
        [
         "2",
         "par contre pour que sunny ne veulent pas que cassie voit c'est souvenir des derniere annee il a vraiment du",
         "on the other hand so that sunny does not want cassie to see it is memory of the last year"
        ],
        [
         "3",
         "c'est pas simplement du a son effacement du destin ils devraient etre en mesure de faire le lien mais si",
         "it is not just because of its erasure of destiny they should be able to make the connection but if"
        ],
        [
         "4",
         "psq la le fait qu'ils oublient a chaque fois",
         "psq the fact that they forget every time"
        ],
        [
         "5",
         "possible",
         "possible"
        ],
        [
         "6",
         "il aurait pas voler des pouvoirs au demon de l'oubli",
         "he would not steal powers from the demon of oblivion"
        ],
        [
         "7",
         "le piaf de merde la",
         "the shitty piaf there"
        ],
        [
         "8",
         "par contre dcp",
         "but dcp"
        ],
        [
         "9",
         "elle sait juste que c'est lui l'anomalie",
         "she just knows he is the anomaly"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fr</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>il a pas pu faire grand chose qui puisse choquer cassie</td>\n",
       "      <td>he could not do much that could shock cassie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jpense juste qu'il aime pas qu'on s'introduise dans sa tete</td>\n",
       "      <td>i just think he does not like us getting into his head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>par contre pour que sunny ne veulent pas que cassie voit c'est souvenir des derniere annee il a vraiment du</td>\n",
       "      <td>on the other hand so that sunny does not want cassie to see it is memory of the last year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c'est pas simplement du a son effacement du destin ils devraient etre en mesure de faire le lien mais si</td>\n",
       "      <td>it is not just because of its erasure of destiny they should be able to make the connection but if</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>psq la le fait qu'ils oublient a chaque fois</td>\n",
       "      <td>psq the fact that they forget every time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>possible</td>\n",
       "      <td>possible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>il aurait pas voler des pouvoirs au demon de l'oubli</td>\n",
       "      <td>he would not steal powers from the demon of oblivion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>le piaf de merde la</td>\n",
       "      <td>the shitty piaf there</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>par contre dcp</td>\n",
       "      <td>but dcp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>elle sait juste que c'est lui l'anomalie</td>\n",
       "      <td>she just knows he is the anomaly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                            fr  \\\n",
       "0                                                      il a pas pu faire grand chose qui puisse choquer cassie   \n",
       "1                                                  jpense juste qu'il aime pas qu'on s'introduise dans sa tete   \n",
       "2  par contre pour que sunny ne veulent pas que cassie voit c'est souvenir des derniere annee il a vraiment du   \n",
       "3     c'est pas simplement du a son effacement du destin ils devraient etre en mesure de faire le lien mais si   \n",
       "4                                                                 psq la le fait qu'ils oublient a chaque fois   \n",
       "5                                                                                                     possible   \n",
       "6                                                         il aurait pas voler des pouvoirs au demon de l'oubli   \n",
       "7                                                                                          le piaf de merde la   \n",
       "8                                                                                               par contre dcp   \n",
       "9                                                                     elle sait juste que c'est lui l'anomalie   \n",
       "\n",
       "                                                                                                   en  \n",
       "0                                                        he could not do much that could shock cassie  \n",
       "1                                              i just think he does not like us getting into his head  \n",
       "2           on the other hand so that sunny does not want cassie to see it is memory of the last year  \n",
       "3  it is not just because of its erasure of destiny they should be able to make the connection but if  \n",
       "4                                                            psq the fact that they forget every time  \n",
       "5                                                                                            possible  \n",
       "6                                                he would not steal powers from the demon of oblivion  \n",
       "7                                                                               the shitty piaf there  \n",
       "8                                                                                             but dcp  \n",
       "9                                                                    she just knows he is the anomaly  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# truncate the sentences to the max_sequence_length\n",
    "dataset['en'] = dataset['en'].apply(lambda x: ' '.join(x.split()[:max_sequence_length]))\n",
    "dataset['fr'] = dataset['fr'].apply(lambda x: ' '.join(x.split()[:max_sequence_length]))\n",
    "\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Sequence Conversion\n",
    "\n",
    "To feed our data to a Seq2Seq model, we will have to convert both the input and the output sentences into integer sequences of fixed length. Check the exploratory data analysis notebook to see the distribution of the lengths of the sentences in the dataset. Based on that, we decided to fix the maximum length of each sentence to 20 since the average length of the sentences in the dataset is around 20.\n",
    "\n",
    "We will use the ``Tokenizer`` class from the ``tensorflow.keras.preprocessing.text`` module to tokenize the text data. The ``Tokenizer`` class will also be used to convert the text to sequences. We will use the ``pad_sequences`` function from the same module to pad the sequences to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(lines, max_vocab_size=5000):\n",
    "    tokenizer = Tokenizer(filters=' ', num_words=max_vocab_size)\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    seq = pad_sequences(seq, maxlen=length, padding='post', truncating='post')\n",
    "    return seq\n",
    "\n",
    "def decode_sequences(tokenizer, sequence):\n",
    "    text = tokenizer.sequences_to_texts([sequence])[0].replace('PAD', '').strip()\n",
    "    return text\n",
    "\n",
    "def get_most_common_words(tokenizer, n=10):\n",
    "    word_counts = sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    return word_counts[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the English sentences\n",
    "eng_tokenizer = tokenization(dataset[\"en\"])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "# Tokenize the French sentences\n",
    "fr_tokenizer = tokenization(dataset[\"fr\"])\n",
    "fr_vocab_size = len(fr_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 6503\n",
      "French Vocabulary Size: 8222\n"
     ]
    }
   ],
   "source": [
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('French Vocabulary Size: %d' % fr_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in English:  [('the', 6224), ('is', 3502), ('i', 3180), ('it', 3036), ('to', 2585), ('of', 2306), ('a', 2271), ('not', 2211), ('that', 2003), ('he', 1722)]\n",
      "Most common words in French:  [('de', 3248), ('le', 2808), ('a', 2658), ('pas', 2262), ('la', 2193), ('que', 2076), ('je', 1799), ('il', 1649), ('ca', 1387), ('un', 1379)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Most common words in English: \", get_most_common_words(eng_tokenizer))\n",
    "print(\"Most common words in French: \", get_most_common_words(fr_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "We will now split the data into train and test set for model training and evaluation, respectively. We will use the ``train_test_split`` function from the ``sklearn.model_selection`` module to split the data. We will use 10% of the data for testing and the rest for training. We will also set the ``random_state`` parameter to 42 to ensure reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to encode the sentences. We will encode French sentences as the input sequences and English sentences as the target sequences. It will be done for both tra and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(fr_tokenizer, max_sequence_length, train_data[\"fr\"])\n",
    "trainY = encode_sequences(eng_tokenizer, max_sequence_length, train_data[\"en\"])\n",
    "\n",
    "# prepare validation data\n",
    "testX = encode_sequences(fr_tokenizer, max_sequence_length, test_data[\"fr\"])\n",
    "testY = encode_sequences(eng_tokenizer, max_sequence_length, test_data[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8949, 20), (8949, 20), (2238, 20), (2238, 20))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape, trainY.shape, testX.shape, testY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the fun part, building the model. We will build a simple Seq2Seq model for text-to-text translation. \n",
    "The model follows a simple architecture:\n",
    "\n",
    "- Input sequence is embedded using an Embedding layer.\n",
    "- The embedded sequence is processed by an LSTM layer to capture context.\n",
    "- Output sequence is generated by repeating and processing with another LSTM layer.\n",
    "- The Dense layer produces a probability distribution over the output vocabulary for each timestep, enabling text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size, units=126):\n",
    "    english_input = Input(shape=input_shape[1:], name=\"input_layer\")  # the shape is (input length x 1) as batchsize excluded\n",
    "\n",
    "    x = LSTM(units, return_sequences=True, activation=\"tanh\", name=\"LSTM_layer\")(english_input)\n",
    "    preds = TimeDistributed(Dense(french_vocab_size, activation=\"softmax\"), name=\"Dense_layer\")(x)\n",
    "    \n",
    "    model = Model(inputs=english_input, outputs=preds, name='simple_seq2seq_model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/rnn.png\"\n",
    "    alt=\"rnn\"\n",
    "    style=\"text-align: center;\" />\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reshape the ``trainX`` and ``trainY`` to be 3-dimensional tensors to be used in the model. The first dimension represents the number of samples (or sentences), the second represents the length of each sequence, and the third represents the number of features in each sequence. We will use the ``trainX`` and ``trainY`` to train the model. We will use the ``testX`` and ``testY`` to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX.reshape((-1, max_sequence_length, 1))\n",
    "trainY = trainY.reshape((trainY.shape[0], trainY.shape[1], 1))\n",
    "\n",
    "testX = testX.reshape((-1, max_sequence_length, 1))\n",
    "testY = testY.reshape((testY.shape[0], testY.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using RMSprop optimizer in this model as it is usually a good choice for recurrent neural networks. We will experiment with other optimizers in the next notebook.\n",
    "\n",
    "We will use the ``sparse_categorical_crossentropy`` loss since we have used integers to encode the target sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"simple_seq2seq_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"simple_seq2seq_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ LSTM_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">64,512</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Dense_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>)       â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">635,000</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m1\u001b[0m)          â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ LSTM_layer (\u001b[38;5;33mLSTM\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m126\u001b[0m)        â”‚        \u001b[38;5;34m64,512\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Dense_layer (\u001b[38;5;33mTimeDistributed\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m5000\u001b[0m)       â”‚       \u001b[38;5;34m635,000\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">699,512</span> (2.67 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m699,512\u001b[0m (2.67 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">699,512</span> (2.67 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m699,512\u001b[0m (2.67 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_model(trainX.shape, max_sequence_length, eng_vocab_size, 5000)\n",
    "\n",
    "rms = optimizers.RMSprop(learning_rate=0.0001)\n",
    "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have used **sparse_categorical_crossentropy** as the loss function because it allows us to use the target sequence as it is instead of one hot encoded format. One hot encoding the target sequences with such a huge vocabulary might consume our system's entire memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we are all set to start training our model. We will train it for **30 epochs** and with a **batch size of 512**. We will also experiment with the hyperparameters in the next notebook.\n",
    "We will also use **ModelCheckpoint()** to save the best model with lowest validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.2952 - loss: 8.3753\n",
      "Epoch 1: val_loss improved from inf to 7.61304, saving model to ../models/baseline.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 85ms/step - accuracy: 0.2963 - loss: 8.3735 - val_accuracy: 0.5141 - val_loss: 7.6130\n",
      "Epoch 2/20\n",
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.5128 - loss: 7.2735\n",
      "Epoch 2: val_loss improved from 7.61304 to 6.13426, saving model to ../models/baseline.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 82ms/step - accuracy: 0.5129 - loss: 7.2703 - val_accuracy: 0.5253 - val_loss: 6.1343\n",
      "Epoch 3/20\n",
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5205 - loss: 5.7956\n",
      "Epoch 3: val_loss improved from 6.13426 to 4.75956, saving model to ../models/baseline.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - accuracy: 0.5205 - loss: 5.7926 - val_accuracy: 0.5310 - val_loss: 4.7596\n",
      "Epoch 4/20\n",
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5291 - loss: 4.5012\n",
      "Epoch 4: val_loss improved from 4.75956 to 3.79251, saving model to ../models/baseline.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - accuracy: 0.5291 - loss: 4.4991 - val_accuracy: 0.5361 - val_loss: 3.7925\n",
      "Epoch 5/20\n",
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5325 - loss: 3.6687\n",
      "Epoch 5: val_loss improved from 3.79251 to 3.32350, saving model to ../models/baseline.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - accuracy: 0.5325 - loss: 3.6677 - val_accuracy: 0.5426 - val_loss: 3.3235\n",
      "Epoch 6/20\n",
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.5401 - loss: 3.2772\n",
      "Epoch 6: val_loss improved from 3.32350 to 3.15332, saving model to ../models/baseline.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - accuracy: 0.5400 - loss: 3.2770 - val_accuracy: 0.5416 - val_loss: 3.1533\n",
      "Epoch 7/20\n",
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.5356 - loss: 3.1630\n",
      "Epoch 7: val_loss improved from 3.15332 to 3.09477, saving model to ../models/baseline.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - accuracy: 0.5355 - loss: 3.1629 - val_accuracy: 0.5406 - val_loss: 3.0948\n",
      "Epoch 8/20\n",
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.5377 - loss: 3.1004\n",
      "Epoch 8: val_loss improved from 3.09477 to 3.06184, saving model to ../models/baseline.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - accuracy: 0.5377 - loss: 3.1005 - val_accuracy: 0.5410 - val_loss: 3.0618\n",
      "Epoch 9/20\n",
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.5344 - loss: 3.0907\n",
      "Epoch 9: val_loss improved from 3.06184 to 3.03852, saving model to ../models/baseline.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - accuracy: 0.5344 - loss: 3.0906 - val_accuracy: 0.5414 - val_loss: 3.0385\n",
      "Epoch 10/20\n",
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.5355 - loss: 3.0678\n",
      "Epoch 10: val_loss improved from 3.03852 to 3.02143, saving model to ../models/baseline.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - accuracy: 0.5355 - loss: 3.0677 - val_accuracy: 0.5412 - val_loss: 3.0214\n",
      "Epoch 11/20\n",
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.5360 - loss: 3.0480\n",
      "Epoch 11: val_loss improved from 3.02143 to 3.00777, saving model to ../models/baseline.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - accuracy: 0.5360 - loss: 3.0480 - val_accuracy: 0.5419 - val_loss: 3.0078\n",
      "Epoch 12/20\n",
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.5298 - loss: 3.0715\n",
      "Epoch 12: val_loss improved from 3.00777 to 2.99699, saving model to ../models/baseline.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - accuracy: 0.5299 - loss: 3.0712 - val_accuracy: 0.5415 - val_loss: 2.9970\n",
      "Epoch 13/20\n",
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.5343 - loss: 3.0397\n",
      "Epoch 13: val_loss improved from 2.99699 to 2.98778, saving model to ../models/baseline.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - accuracy: 0.5343 - loss: 3.0395 - val_accuracy: 0.5440 - val_loss: 2.9878\n",
      "Epoch 14/20\n",
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.5326 - loss: 3.0520\n",
      "Epoch 14: val_loss improved from 2.98778 to 2.98031, saving model to ../models/baseline.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 84ms/step - accuracy: 0.5326 - loss: 3.0516 - val_accuracy: 0.5439 - val_loss: 2.9803\n",
      "Epoch 15/20\n",
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.5396 - loss: 3.0084\n",
      "Epoch 15: val_loss improved from 2.98031 to 2.97373, saving model to ../models/baseline.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 84ms/step - accuracy: 0.5396 - loss: 3.0084 - val_accuracy: 0.5436 - val_loss: 2.9737\n",
      "Epoch 16/20\n",
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.5419 - loss: 2.9869\n",
      "Epoch 16: val_loss improved from 2.97373 to 2.96790, saving model to ../models/baseline.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - accuracy: 0.5418 - loss: 2.9870 - val_accuracy: 0.5447 - val_loss: 2.9679\n",
      "Epoch 17/20\n",
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.5422 - loss: 2.9815\n",
      "Epoch 17: val_loss improved from 2.96790 to 2.96302, saving model to ../models/baseline.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 84ms/step - accuracy: 0.5421 - loss: 2.9816 - val_accuracy: 0.5438 - val_loss: 2.9630\n",
      "Epoch 18/20\n",
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.5417 - loss: 2.9792\n",
      "Epoch 18: val_loss improved from 2.96302 to 2.95811, saving model to ../models/baseline.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 83ms/step - accuracy: 0.5417 - loss: 2.9793 - val_accuracy: 0.5451 - val_loss: 2.9581\n",
      "Epoch 19/20\n",
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.5430 - loss: 2.9630\n",
      "Epoch 19: val_loss improved from 2.95811 to 2.95387, saving model to ../models/baseline.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 82ms/step - accuracy: 0.5430 - loss: 2.9632 - val_accuracy: 0.5455 - val_loss: 2.9539\n",
      "Epoch 20/20\n",
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5381 - loss: 2.9925\n",
      "Epoch 20: val_loss improved from 2.95387 to 2.95007, saving model to ../models/baseline.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 82ms/step - accuracy: 0.5381 - loss: 2.9923 - val_accuracy: 0.5456 - val_loss: 2.9501\n"
     ]
    }
   ],
   "source": [
    "filename = '../models/baseline.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "history = model.fit(trainX, trainY, \n",
    "          epochs=20, batch_size=64, \n",
    "          validation_split=0.2,\n",
    "          callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = load_model('../models/baseline.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "Now that we have our model, let's make some predictions. We will create a function called ``translate`` which will take a sentence in English as input and return the translated sentence in French. We will use the trained model to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before let's test on the predictions classes to see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_to_predict = 10\n",
    "\n",
    "# Make predictions on the subset\n",
    "subset_to_predict = testX[:size_to_predict]\n",
    "predictions = model.predict_on_batch(subset_to_predict)\n",
    "predictions_classes = np.argmax(predictions, axis=-1)\n",
    "\n",
    "# reshape the subset to predict and the testY to be able to decode them\n",
    "reshapedX_subset = subset_to_predict.reshape((subset_to_predict.shape[0], subset_to_predict.shape[1]))\n",
    "reshapedY_subset = testY[:size_to_predict].reshape((testY[:size_to_predict].shape[0], testY[:size_to_predict].shape[1]))\n",
    "\n",
    "predicted_df = pd.DataFrame(columns=['french_sentence', 'actual_english_sentence', 'predicted_english_sentence'])\n",
    "\n",
    "i = 0\n",
    "for seq in predictions_classes:\n",
    "    predicted_text = decode_sequences(eng_tokenizer, seq)\n",
    "    original_french_sentence = decode_sequences(fr_tokenizer, reshapedX_subset[i])\n",
    "    original_english_sentence = decode_sequences(eng_tokenizer, reshapedY_subset[i])\n",
    "    \n",
    "    predicted_df.loc[i] = [original_french_sentence, original_english_sentence, predicted_text]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "french_sentence",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "actual_english_sentence",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "predicted_english_sentence",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "7a61e2f1-a32d-4a59-be83-0c0addc98e3c",
       "rows": [
        [
         "0",
         "ligne du dieu des tempete sah",
         "line of the god of tempete sah",
         "i the the the"
        ],
        [
         "1",
         "oui ca pas de doute",
         "yeah no doubt",
         "i the the"
        ],
        [
         "2",
         "bah oe sinon c'est pas drole",
         "well otherwise it is not funny",
         "i i i the the"
        ],
        [
         "3",
         "psq t'as pas lu encore",
         "psq you have not read kingdom yet",
         "i i"
        ],
        [
         "4",
         "tout est explique dans l'arc si jamais t'as d'autres questions hesite pas",
         "everything is explained in the bow if you ever have other questions hesite not",
         "i the the the"
        ],
        [
         "5",
         "quitte a les quand tu les en francais dans quelque jours",
         "even read them again when you get them out in french in a few days",
         "i the the the the the the"
        ],
        [
         "6",
         "justement je pense pas",
         "i do not think so",
         "i the the"
        ],
        [
         "7",
         "ca va etre nimporte quoi vu le nombre de creature corrompu et superieur quil a en stock",
         "it is going to be anything given the number of corrupted and superior creatures it has in stock",
         "i is is the the is the the"
        ],
        [
         "8",
         "ouais donc cest uniquement les premiers cas par le sortilege sur la lune pas les premiers portes ni meme",
         "yeah so it is only the first cases of injection by the exitlege on the moon not the first doors",
         "i is is the the the the the the"
        ],
        [
         "9",
         "deja il faut qu'elle lui trouve un nom",
         "she has to find her name",
         "i the the the"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>french_sentence</th>\n",
       "      <th>actual_english_sentence</th>\n",
       "      <th>predicted_english_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ligne du dieu des tempete sah</td>\n",
       "      <td>line of the god of tempete sah</td>\n",
       "      <td>i the the the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oui ca pas de doute</td>\n",
       "      <td>yeah no doubt</td>\n",
       "      <td>i the the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bah oe sinon c'est pas drole</td>\n",
       "      <td>well otherwise it is not funny</td>\n",
       "      <td>i i i the the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>psq t'as pas lu encore</td>\n",
       "      <td>psq you have not read kingdom yet</td>\n",
       "      <td>i i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tout est explique dans l'arc si jamais t'as d'autres questions hesite pas</td>\n",
       "      <td>everything is explained in the bow if you ever have other questions hesite not</td>\n",
       "      <td>i the the the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quitte a les quand tu les en francais dans quelque jours</td>\n",
       "      <td>even read them again when you get them out in french in a few days</td>\n",
       "      <td>i the the the the the the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>justement je pense pas</td>\n",
       "      <td>i do not think so</td>\n",
       "      <td>i the the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ca va etre nimporte quoi vu le nombre de creature corrompu et superieur quil a en stock</td>\n",
       "      <td>it is going to be anything given the number of corrupted and superior creatures it has in stock</td>\n",
       "      <td>i is is the the is the the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ouais donc cest uniquement les premiers cas par le sortilege sur la lune pas les premiers portes ni meme</td>\n",
       "      <td>yeah so it is only the first cases of injection by the exitlege on the moon not the first doors</td>\n",
       "      <td>i is is the the the the the the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>deja il faut qu'elle lui trouve un nom</td>\n",
       "      <td>she has to find her name</td>\n",
       "      <td>i the the the</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                            french_sentence  \\\n",
       "0                                                                             ligne du dieu des tempete sah   \n",
       "1                                                                                       oui ca pas de doute   \n",
       "2                                                                              bah oe sinon c'est pas drole   \n",
       "3                                                                                    psq t'as pas lu encore   \n",
       "4                                 tout est explique dans l'arc si jamais t'as d'autres questions hesite pas   \n",
       "5                                                  quitte a les quand tu les en francais dans quelque jours   \n",
       "6                                                                                    justement je pense pas   \n",
       "7                   ca va etre nimporte quoi vu le nombre de creature corrompu et superieur quil a en stock   \n",
       "8  ouais donc cest uniquement les premiers cas par le sortilege sur la lune pas les premiers portes ni meme   \n",
       "9                                                                    deja il faut qu'elle lui trouve un nom   \n",
       "\n",
       "                                                                           actual_english_sentence  \\\n",
       "0                                                                   line of the god of tempete sah   \n",
       "1                                                                                    yeah no doubt   \n",
       "2                                                                   well otherwise it is not funny   \n",
       "3                                                                psq you have not read kingdom yet   \n",
       "4                   everything is explained in the bow if you ever have other questions hesite not   \n",
       "5                               even read them again when you get them out in french in a few days   \n",
       "6                                                                                i do not think so   \n",
       "7  it is going to be anything given the number of corrupted and superior creatures it has in stock   \n",
       "8  yeah so it is only the first cases of injection by the exitlege on the moon not the first doors   \n",
       "9                                                                         she has to find her name   \n",
       "\n",
       "        predicted_english_sentence  \n",
       "0                    i the the the  \n",
       "1                        i the the  \n",
       "2                    i i i the the  \n",
       "3                              i i  \n",
       "4                    i the the the  \n",
       "5        i the the the the the the  \n",
       "6                        i the the  \n",
       "7       i is is the the is the the  \n",
       "8  i is is the the the the the the  \n",
       "9                    i the the the  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
