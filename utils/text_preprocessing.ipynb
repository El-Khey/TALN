{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2722fc75",
   "metadata": {},
   "source": [
    "# Text Data Preprocessing Pipeline\n",
    "\n",
    "This notebook implements a comprehensive text preprocessing pipeline for natural language processing tasks. The pipeline handles data loading, text extraction, and various normalization steps to prepare textual data for further analysis or model training.\n",
    "\n",
    "## Objective\n",
    "- Load and consolidate text data from multiple CSV files\n",
    "- Clean and normalize text content \n",
    "- Prepare standardized dataset for NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4c38f4",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "\n",
    "- Enables autoreload extension to automatically reload modified modules\n",
    "- Imports the custom TextProcessor class from utils\n",
    "- Loads required data manipulation libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc8922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%aimport utils.text_processing\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b7945c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_processing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextProcessor\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "from utils.text_processing import TextProcessor\n",
    "import pandas as pd\n",
    "import requests\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604c245",
   "metadata": {},
   "source": [
    "## Load CSV Files\n",
    "\n",
    "The pipeline scans for CSV files in the data directory:\n",
    "1. Uses glob to find all .csv files\n",
    "2. Validates file structure by inspecting first file\n",
    "3. Ensures consistency in data format across files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a4fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = glob.glob('data/raw/*.csv')\n",
    "print(f'Found {len(csv_files)} CSV files')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1dc66c",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "For each CSV file:\n",
    "1. Extracts the 'content' column containing text messages\n",
    "2. Removes empty entries\n",
    "3. Maintains data quality by filtering invalid entries\n",
    "4. Tracks processing statistics for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e57b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store all DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Process each CSV file\n",
    "for file in csv_files:\n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # keep only the content column\n",
    "    if 'content' not in df.columns:\n",
    "        print(f'Skipping {file}: no content column')\n",
    "        continue\n",
    "    \n",
    "    df = df[['content']]\n",
    "    df = df[df['content'].str.len() > 0]\n",
    "    df.rename(columns={'content': 'fr'}, inplace=True)\n",
    "    \n",
    "    dataframes.append(df)\n",
    "    print(f'Processed {file}: {len(df)} messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2778d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "final_df = final_df.drop_duplicates()\n",
    "final_df['fr'] = TextProcessor(final_df, 'fr').transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae82c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_batch(texts, source=\"fr\", target=\"en\", url=\"http://127.0.0.1:5000/translate\"):\n",
    "    payload = {\n",
    "        \"q\": texts,\n",
    "        \"source\": source,\n",
    "        \"target\": target\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return [item for item in response.json()[\"translatedText\"]]\n",
    "\n",
    "def translate_column(df, column, batch_size=50):\n",
    "    translations = []\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df[column].iloc[i:i + batch_size].tolist()\n",
    "        batch = [text.replace('<start>', '').replace('<end>', '') for text in batch]\n",
    "        \n",
    "        translated = translate_batch(batch)\n",
    "        translations.extend(translated)\n",
    "    \n",
    "    return translations\n",
    "\n",
    "final_df['en'] = translate_column(final_df, 'fr')\n",
    "final_df['en'] = TextProcessor(final_df, 'en').transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b617efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape the final DataFrame\n",
    "final_df = final_df[['fr', 'en']]\n",
    "final_df = final_df.drop_duplicates()\n",
    "final_df = final_df[final_df['fr'].str.len() > 0]\n",
    "final_df = final_df[final_df['en'].str.len() > 0]\n",
    "final_df = final_df.dropna()\n",
    "final_df = final_df.reset_index(drop=True)\n",
    "print(f'Final DataFrame shape: {final_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecf1cdc",
   "metadata": {},
   "source": [
    "## Data Export\n",
    "\n",
    "Final processing steps:\n",
    "1. Combines all processed DataFrames\n",
    "2. Removes any duplicate entries\n",
    "3. Exports to CSV format for downstream tasks\n",
    "4. Preserves both original and processed versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04396d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = './data/cleaned/fr_en_processed_data.csv'\n",
    "final_df.to_csv(output_file, index=False)\n",
    "print(f'\\nProcessed data saved to {output_file}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
