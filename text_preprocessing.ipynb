{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2722fc75",
   "metadata": {},
   "source": [
    "# Text Data Preprocessing Pipeline\n",
    "\n",
    "This notebook implements a comprehensive text preprocessing pipeline for natural language processing tasks. The pipeline handles data loading, text extraction, and various normalization steps to prepare textual data for further analysis or model training.\n",
    "\n",
    "## Objective\n",
    "- Load and consolidate text data from multiple CSV files\n",
    "- Clean and normalize text content \n",
    "- Prepare standardized dataset for NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4c38f4",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "\n",
    "- Enables autoreload extension to automatically reload modified modules\n",
    "- Imports the custom TextProcessor class from utils\n",
    "- Loads required data manipulation libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc8922",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport utils.text_processing\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7945c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.text_processing import TextProcessor\n",
    "import pandas as pd\n",
    "import requests\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604c245",
   "metadata": {},
   "source": [
    "## Load CSV Files\n",
    "\n",
    "The pipeline scans for CSV files in the data directory:\n",
    "1. Uses glob to find all .csv files\n",
    "2. Validates file structure by inspecting first file\n",
    "3. Ensures consistency in data format across files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a4fd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 CSV files\n"
     ]
    }
   ],
   "source": [
    "csv_files = glob.glob('data/raw/*.csv')\n",
    "print(f'Found {len(csv_files)} CSV files')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1dc66c",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "For each CSV file:\n",
    "1. Extracts the 'content' column containing text messages\n",
    "2. Removes empty entries\n",
    "3. Maintains data quality by filtering invalid entries\n",
    "4. Tracks processing statistics for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e57b69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data/raw/shadow-slave_page_24.csv: 996 messages\n",
      "Processed data/raw/shadow-slave_page_17.csv: 991 messages\n",
      "Processed data/raw/shadow-slave_page_25.csv: 993 messages\n",
      "Processed data/raw/shadow-slave_page_23.csv: 989 messages\n",
      "Processed data/raw/shadow-slave_page_18.csv: 995 messages\n",
      "Processed data/raw/shadow-slave_page_21.csv: 990 messages\n",
      "Processed data/raw/shadow-slave_page_16.csv: 984 messages\n",
      "Processed data/raw/shadow-slave_page_19.csv: 990 messages\n",
      "Processed data/raw/shadow-slave_page_20.csv: 991 messages\n",
      "Processed data/raw/shadow-slave_page_26.csv: 994 messages\n",
      "Processed data/raw/shadow-slave_page_27.csv: 992 messages\n",
      "Processed data/raw/shadow-slave_page_22.csv: 994 messages\n",
      "Processed data/raw/shadow-slave_page_28.csv: 599 messages\n"
     ]
    }
   ],
   "source": [
    "# Initialize list to store all DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Process each CSV file\n",
    "for file in csv_files:\n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # keep only the content column\n",
    "    if 'content' not in df.columns:\n",
    "        print(f'Skipping {file}: no content column')\n",
    "        continue\n",
    "    \n",
    "    df = df[['content']]\n",
    "    df = df[df['content'].str.len() > 0]\n",
    "    df.rename(columns={'content': 'fr'}, inplace=True)\n",
    "    \n",
    "    dataframes.append(df)\n",
    "    print(f'Processed {file}: {len(df)} messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2778d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "final_df = final_df.drop_duplicates()\n",
    "final_df['fr'] = TextProcessor(final_df, 'fr').transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae82c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_batch(texts, source=\"fr\", target=\"en\", url=\"http://127.0.0.1:5000/translate\"):\n",
    "    payload = {\n",
    "        \"q\": texts,\n",
    "        \"source\": source,\n",
    "        \"target\": target\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return [item for item in response.json()[\"translatedText\"]]\n",
    "\n",
    "def translate_column(df, column, batch_size=50):\n",
    "    translations = []\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df[column].iloc[i:i + batch_size].tolist()\n",
    "        batch = [text.replace('<start>', '').replace('<end>', '') for text in batch]\n",
    "        \n",
    "        translated = translate_batch(batch)\n",
    "        translations.extend(translated)\n",
    "    \n",
    "    return translations\n",
    "\n",
    "final_df['en'] = translate_column(final_df, 'fr')\n",
    "final_df['en'] = TextProcessor(final_df, 'en').transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b617efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame shape: (11187, 2)\n"
     ]
    }
   ],
   "source": [
    "# shape the final DataFrame\n",
    "final_df = final_df[['fr', 'en']]\n",
    "final_df = final_df.drop_duplicates()\n",
    "final_df = final_df[final_df['fr'].str.len() > 0]\n",
    "final_df = final_df[final_df['en'].str.len() > 0]\n",
    "final_df = final_df.dropna()\n",
    "final_df = final_df.reset_index(drop=True)\n",
    "print(f'Final DataFrame shape: {final_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecf1cdc",
   "metadata": {},
   "source": [
    "## Data Export\n",
    "\n",
    "Final processing steps:\n",
    "1. Combines all processed DataFrames\n",
    "2. Removes any duplicate entries\n",
    "3. Exports to CSV format for downstream tasks\n",
    "4. Preserves both original and processed versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04396d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed data saved to ./data/cleaned/fr_processed_data.csv\n"
     ]
    }
   ],
   "source": [
    "output_file = './data/cleaned/fr_en_processed_data.csv'\n",
    "final_df.to_csv(output_file, index=False)\n",
    "print(f'\\nProcessed data saved to {output_file}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
